- [ ] simple interface for sync/async steaming/not llm calls
- [ ] first class support for OpenAI
- [ ] first class support for Anthropic

----------------
- [ ] support other models through LiteLLM
- [ ] token counting
- [ ] input / output formats for maximum convenience
- [ ] auto rate limiting
- [ ] error handling, retries
- [ ] logging
- [ ] loading prompts
- [ ] easy config - model aliases
- [ ] cost tracking
- [ ] prompt rendering *within* SpiceMessages inputs? i.e. parts of messages can be rendered at call time?
- [ ] support response formats
- [ ] support images
- [ ] support temperature 
- [ ] measure timing metrics
